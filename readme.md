## О сервере
Clickhouse и Airflow размещены на моем Docker Swarm кластере в отдельном стеке, оба на отдельной машине, где ничего больше нет. Доступ к сервисам через https://uchi-af.kkmagician.com для Airflow и https://uchi-ch.kkmagician.com для Clickhouse. Для скриптов Airflow используются Docker Secrets, чтобы не палить пароли в гите.
Оба сервиса сидят за reverse proxy Traefik кластера. Для Airlow стоит basic auth, для Clickhouse созданы два юзера: мой и юзер проверяющего, права одинаковые. Вся активность записывается :)
Если интересно, настройка стека для кластера лежит в swarm.yml.

## О задании
Описание продакшн-таблицы лежит в комментарии над запросом для ее создания в файле production.sql. Airflow в папке airflow.
Если данные действительно идут из Kafka, можно было бы воспользоваться специализированным движком (https://clickhouse.yandex/docs/en/operations/table_engines/kafka/) или по необходиимости своим сервисом, который бы слушал топики и валидировал/лил данные. Если у нас один файлик на 250 Мб раз в день, то я бы не мучался с временной таблицей/продакшн таблицей, а подготовил бы данные и залил сразу куда надо по HTTP. Все сильно зависит от того, насколько "чистыми" приходят данные, стоит ли ожидать ошибок внутри, как часто обновляется схема, напрмер, добавляются столбцы. Дополнительные middlware-сервисы между исходными данными и CH нужны скорее в том случае, когда мы не доверяем источнику или хотим совершать какие-то специфические преобразования. В данном задании таким преобразованием мог бы быть парсинг userAgent на браузер (если это браузер), ОС, и насколько браузер устаревший. Юз-кейс для этого такой: когда надо выкатить новую фичу с библиотекой, которая не поддерживается в старых браузерах, обычно идут в аналитику узнавать, сколько пользователей до сих пор сидят на старых версиях.

В данном примере работаю с движком Log – заливаю файл через curl в именованную таблицу по дню. Он не настолько большой, чтобы возиться с правами доступа и перекидками между серверами (в нашем случае - контейнерами) с движком File. Документация не рекомендует заливать больше 1 млн строк, но у нас меньше :) Подход не супер масштабиируемый, как мне кажется, но на текущих размерах ОК.

## Об аналитике
Файл analytics.py. Запрос не стал разделять на две части, просто использовал WITH TOTALS. Подход с groupUniqArray показал себя лучше по памяти, чем LIMIT 1 BY. В целом, на таких объемах даже на слабенькой тачке запросы делаются мгновенно. Юзеров маловато, так что гипотеза так себе. Ничего сверхестественного для проверки не делал: binom из scipy, т.к. у нас два варианта: остался пользователь или нет. Из запроса вырезал пользователей, у которых первый день активности меньше чем за 7 дней до последней даты в датасете: было бы нечестно их считать в любой из групп.
